{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38977aea-492f-4394-9ddb-c56ab2ef89d1",
   "metadata": {},
   "source": [
    "# Using Primer Engines and NLP to get a big-picture view across documents\n",
    "\n",
    "Making sense of the contents of a large set of unknown documents is relevant to many industry applications. This tutorial will take you through the HierarchicalTagger, a pipeline created to address this broad use case. \n",
    "\n",
    "Combining the power of [Primer Engines](https://developers.primer.ai/docs) with a custom prototype built on top of deep NLP models, the pipeline is designed to ingest an arbitrary set of documents, produce a hierarchical visualization of their contents (like the one below), and finally make the corpus searchable by tagging each document with both specific and broad keywords.\n",
    "\n",
    "You can follow the full tutorial [here](ADDLINK)\n",
    "\n",
    "![](./example-sunburst-chart.png)\n",
    "\n",
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13fffeef-c564-4d9a-97c4-417ee21387b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from more_itertools import chunked\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8549c53b-30df-4b24-948a-551572349cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to the repository root. Apply os.path.dirname twice to go up two levels.\n",
    "# All paths will be expressed relative to the directory root.\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.realpath(\"__file__\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f53b71-7506-42bd-845e-e5b60ecf8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you launched the Jupyter notebook with `$PYTHONPATH=$(pwd) jupyter notebook\n",
    "# This allows importing python modules found at the root of the repository like this:\n",
    "from engines_utils.engines import infer_model_on_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5810185-ac58-42c4-9c67-ab9b3b26fe25",
   "metadata": {},
   "source": [
    "## Generate Abstractive Topics via Engines\n",
    "\n",
    "We would now hit the Primer APIs with batches of documents for processing and receive the generated topic labels results back. So that you can proceed directly to the next steps, weâ€™ve done this for you, and included the processed results for a random sample of 3000 products in `PRECOMPUTED_ITEM_TOPICS` path. Feel free to save your Engines credits and proceed to the next section.\n",
    "\n",
    "Running the pipeline on your own data is easy and the following cells show how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19ffff2-7b46-441d-b6b5-870e281934a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by loading the Amazon Product Dateset into a pandas DataFrame\n",
    "PATH_TO_DATASET = os.path.join(ROOT_DIR, \"./examples/data/amazon_products_2020.csv\")\n",
    "df = pd.read_csv(PATH_TO_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the dataset looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac83552e-b700-4d8e-a5ef-dc2ebb29268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We limit our analysis to 3000 documents to limit the computational burden.\n",
    "# Random sampling guarantees that the documents remain representative of the whole dataset\n",
    "# We fix a random_state parameter so that the same 3000 documents are chosen each time one runs this notebook.\n",
    "sampled_items = df[df['About Product'].notnull()].sample(3000, random_state=1363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a8f26c2-a15b-4e9f-bd77-3ae3b53f4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform documents in standard format to send to Engines: a list of dictionaries with an `id` and a `text` key. \n",
    "documents = [{\"id\": r[\"Uniq Id\"], \"text\": r['About Product']} for i, r in sampled_items.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea361fe-103c-4f92-b670-9673f1a99b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm we have 3000 documents\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c21c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a dictionary storing additional attributes about the documents. This will be necessary to \n",
    "# to display the original document data in the webapp we will use later.\n",
    "document_attributes = {r[\"Uniq Id\"]: {\"title\": r['Product Name'], \"text\": r['About Product']} for i, r in sampled_items.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd1aa818-394c-42b1-b405-02b4c3a37717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stored our API key in credentials.py in the root of the repository and outside of version control.\n",
    "# It contains a single line:\n",
    "# ENGINES_API_KEY=\"YOUR_ENGINES_API_KEY\"\n",
    "# Please do the same if you wish to try it out. Otherwise, skip to the next section.\n",
    "# We can then import the key without exposing it into the notebook\n",
    "from credentials import ENGINES_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08cc9e-595c-4cdc-992c-72ada4ab0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try pinging the the Engines API and check everthing is in good shape\n",
    "# We'll just try out with 2 test documents\n",
    "test_documents = documents[:2]\n",
    "topics_results = await infer_model_on_docs(test_documents, \n",
    "                                               model_name=\"abstractive_topics\", \n",
    "                                               api_key=ENGINES_API_KEY, \n",
    "                                               batch_size=10,\n",
    "                                               **{\"segmented\": False})\n",
    "# The results will be a dictionary mapping document id to the response from the API\n",
    "# Let's check we are happy with what that looks like.\n",
    "topics_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b8929-d97c-4dd1-a491-ea63b7e6825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now ready to send the full 3000 documents to the Engines API and save the results.\n",
    "ITEM_TOPICS = os.path.join(ROOT_DIR, \"./examples/data/amazon_products.json\")\n",
    "\n",
    "topics = {}\n",
    "\n",
    "# Infer topics from Engines\n",
    "for doc_chunk in chunked(documents, 100):\n",
    "    topics_results = await infer_model_on_docs(doc_chunk, \n",
    "                                               model_name=\"abstractive_topics\", \n",
    "                                               api_key=ENGINES_API_KEY, \n",
    "                                               batch_size=10,\n",
    "                                               **{\"segmented\": False})\n",
    "    topics.update(topics_results)\n",
    "    clear_output()\n",
    "    print(f\"Collected topics for {len(topics)} documents\")\n",
    "    # Save\n",
    "    with open(ITEM_TOPICS, \"w\") as f:\n",
    "        json.dump(topics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374681ca-d631-457f-807e-496b49412243",
   "metadata": {},
   "source": [
    "## Ingest the processed docs into the HierarchicalTagger pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b20575eb-a666-4c96-9521-11f0d9be8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic labels from precomputed datast\n",
    "# If you ran Engines on your own data, change the path to where you stored the output.\n",
    "PRECOMPUTED_ITEM_TOPICS = os.path.join(ROOT_DIR, \"./examples/data/amazon_products_precomputed.json\")\n",
    "with open(PRECOMPUTED_ITEM_TOPICS, \"r\") as f:\n",
    "    topics = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd4f463-1aad-47f2-a7dd-ffbf08ad23ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm we have topics for all 3000 documents\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect results for an item. We are interested in the `topics` key\n",
    "topics[\"96d96237978ba26bbc6baa437372527a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c02e44-f246-4368-be6d-b0328ca826ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module and create a HierarchicalTagger instance\n",
    "from hierarchical_tagger.hierarchical_tagger import HierarchicalTagger\n",
    "hierarchical_tagger = HierarchicalTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6edbc84-93f1-40f9-ad98-3a066a387273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rework the document topic representations into a dictionary mapping document_id: List[topic labels as str]\n",
    "document_topics = {document_id: topics_entry['topics'] for document_id, topics_entry in topics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d40d3e-7de5-43d9-aa7a-b5152934b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the document ids and their corresponding topic labels for ingest\n",
    "hierarchical_tagger.ingest(document_terms=document_topics, document_attributes=document_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3d94ed6-79fb-4af5-af40-9216a2cf7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous step is the most computationally demanding.\n",
    "# To avoid having to repeat this, we can save our HierarchicalTagger instance to a json file, using the .to_json() helper method.\n",
    "# As this file will also be the input data to our web app, so let's save it in `webapp/data/`\n",
    "SERIALIZED_INSTANCE_PATH = os.path.join(ROOT_DIR, \"./webapp/data/amazon_products.json\")\n",
    "with open(SERIALIZED_INSTANCE_PATH, \"w\") as f:\n",
    "    f.write(hierarchical_tagger.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d799f220-f031-40f8-9b74-4e6eba99b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we can reload a HierarchicalTagger instance from file\n",
    "with open(SERIALIZED_INSTANCE_PATH, \"r\") as f:\n",
    "    reloaded_serialized =  json.load(f)\n",
    "hierarchical_tagger = HierarchicalTagger.from_dict(reloaded_serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88772d9c-5abb-483f-b0ff-016cf46ec9fa",
   "metadata": {},
   "source": [
    "## Build the topic tree and tag the documents\n",
    "\n",
    "#### Hierarchical topic tree\n",
    "\n",
    "The `.fit_tag_tree()` method populates the `.tree` attribute with a [treelib](https://treelib.readthedocs.io/en/latest/) object representing the extracted term tree. This can be manipulated and explored with all the treelib methods, for example `.show()` to print out a text representation of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tree\n",
    "hierarchical_tagger.fit_tag_tree()\n",
    "hierarchical_tagger.tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04563b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final step is tagging the original documents based on the hierarchy we found in the tree\n",
    "hierarchical_tagger.tag_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48bad2-ac65-440d-8da4-e8843bca49b8",
   "metadata": {},
   "source": [
    "#### Document tags\n",
    "\n",
    "Inspect the `document_tags` attribute: a dictionary mapping document `id` to a list of tuples of the form `(term, score, node_id)` sorted by descending score. `score` measures how close in meaning the term is to the document. We would expect higher level abstractions to have lower scores.\n",
    "`node_id` loosely indicates how high the node is in the tree: it's not a perfect measure, but more abstract terms will generally have higher `node_id`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d209692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the tags assinged to the Hover Board example we saw above\n",
    "hierarchical_tagger.document_tags[\"96d96237978ba26bbc6baa437372527a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dececf3-75dd-4231-b984-08b1b62ab825",
   "metadata": {},
   "source": [
    "## Tuning parameters and using the web app\n",
    "\n",
    "We expose several tuning parameters that the investigator can tweak to guide the extraction of the term tree and the logic applied when tagging the documents. You can find detailed documentation [here](https://github.com/PrimerAI/hierarchical-tagger/blob/main/hierarchical_tagger/hierarchical_tagger.py#L33). \n",
    "\n",
    "This repository also includes a simple web app to facilitate this iterative exploration. In a terminal, just run activate this tutorial's virtual enviroment and run the app like this:\n",
    "\n",
    "```\n",
    "$ workon ht-repo # Or alternative command to activate your virtual environment\n",
    "$ streamlit run webapp/app.py\n",
    "```\n",
    "\n",
    "You can find more detailed instructions in our [companion tutorial](TODO_ADDLINK).\n",
    "\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "We hope you enjoyed this tutorial! Can you think of any other data you could run this on?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21aeb78-746f-4753-a8c6-4064f7cbbbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
